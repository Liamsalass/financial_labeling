{"cells":[{"cell_type":"markdown","metadata":{"id":"2rdkn_lnJToq"},"source":["### RQ3 Training Code\n","Took rq3_train.py and repurposed it for Google Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1713746019733,"user":{"displayName":"Leo Sandler","userId":"07056353084122843132"},"user_tz":240},"id":"yTSJrsrAJtAG"},"outputs":[],"source":["installed = True\n","mounted = True\n","\n","if installed == False:\n","     # Install missing packages from Colab Environment\n","     !pip install datasets\n","     !pip install torch_optimizer\n","     !pip install evaluate\n","     !pip install peft\n","\n","if mounted == False:\n","     # Mount Google Drive\n","     from google.colab import drive\n","     drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6765,"status":"ok","timestamp":1713746026738,"user":{"displayName":"Leo Sandler","userId":"07056353084122843132"},"user_tz":240},"id":"paTtpHaaJTos"},"outputs":[],"source":["import os\n","import datasets\n","import torch\n","import torch_optimizer\n","from copy import deepcopy\n","from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer, AutoModelForTokenClassification, AutoTokenizer, TrainerCallback, BitsAndBytesConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1713746026739,"user":{"displayName":"Leo Sandler","userId":"07056353084122843132"},"user_tz":240},"id":"L55ahQ4SJTot"},"outputs":[],"source":["class CustomCallback(TrainerCallback):\n","     \"\"\"\n","     This custom callback ensures that training metrics are included in the Hugging Face trainer_state.json logs.\n","     From: https://discuss.huggingface.co/t/metrics-for-training-set-in-trainer/2461/7\n","     \"\"\"\n","     def __init__(self, trainer) -> None:\n","          super().__init__()\n","          self._trainer = trainer\n","\n","     def on_epoch_end(self, args, state, control, **kwargs):\n","          if control.should_evaluate:\n","               control_copy = deepcopy(control)\n","               self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n","               return control_copy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":886},"executionInfo":{"elapsed":1436679,"status":"error","timestamp":1713748616939,"user":{"displayName":"Leo Sandler","userId":"07056353084122843132"},"user_tz":240},"id":"qaFbHFd3JTou","outputId":"85eed18b-8584-4864-87c5-a577164e6fb8"},"outputs":[],"source":["import os\n","project_path = r\"/content/drive/MyDrive/financial_labeling\" \n","os.chdir(project_path)\n","print(os.getcwd())\n","# Change directory so RQ3 utils can be imported\n","# TODO: Change location and drive so it can be mounted directly?\n","\n","from rq3_utils import compute_metrics\n","\n","model_name = \"MobileBERT\"\n","subset_size = -1\n","checkpoint_path = \"rq3_mobilebert_model\"\n","resume_from_checkpoint_path = None\n","learning_rate = 2e-5\n","train_batch_size_per_device = 32\n","val_batch_size_per_device = 16\n","epochs = 1\n","use_peft = 0\n","quantize = 0\n","\n","# TODO: Find a way to match code with train? Arguments are hardcoded instead of taken from the command line, but rest is the same\n","\n","print(\"CUDA available: \", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","     print(\"CUDA current device: \", torch.cuda.current_device())  # CPU is -1. Else GPU\n","     # FP4 requires GPU, setting model quantization config accordingly\n","     if use_peft == 1 and quantize == 1:\n","          # NOTE: Quantization is causing some very odd behaviour with NaN loss and high 90s accuracy. Not working currently\n","          bnb_config = BitsAndBytesConfig(\n","               load_in_4bit=True,\n","               bnb_4bit_use_double_quant=True,\n","               bnb_4bit_quant_type=\"nf4\",\n","               bnb_4bit_compute_dtype=torch.bfloat16\n","          )\n","     else:\n","          bnb_config = None\n","else:\n","     print(\"CUDA unavailable, using CPU\")\n","     bnb_config = None\n","\n","if bnb_config is not None:\n","     print(\"Performing FP4 quantization during training.\")\n","\n","# Load the train and val dataset splits.\n","print(\"Loading train dataset.\")\n","train_dataset = datasets.load_dataset(\"nlpaueb/finer-139\", split=\"train\")\n","print(\"Train dataset loaded. Loading val dataset.\")\n","val_dataset = datasets.load_dataset(\"nlpaueb/finer-139\", split=\"validation\")\n","print(\"Val dataset loaded\")\n","\n","# Verifying command line args\n","assert model_name in [\"MobileBERT\", \"SEC-BERT-BASE\", \"SEC-BERT-NUM\", \"SEC-BERT-SHAPE\"]\n","\n","if subset_size != -1:\n","     assert 0 < subset_size < len(val_dataset)\n","     # Selects the specified # of samples from the subset argument.\n","     train_dataset = train_dataset.select(range(subset_size))\n","     val_dataset = val_dataset.select(range(subset_size))\n","     print(\"Training \" + model_name + \" on a subset of FiNER-139 train/val sets with \" + str(subset_size) + \" samples each.\")\n","else:\n","     print(\"Training \" + model_name + \" on full FiNER-139 train/val sets with \" + str(len(train_dataset)) + \" train samples and \" + str(len(val_dataset)) + \" val samples.\")\n","\n","full_checkpoint_path = os.getcwd() + \"/\" + checkpoint_path\n","if os.path.isdir(full_checkpoint_path) is False:\n","     os.mkdir(checkpoint_path)\n","assert os.path.isdir(checkpoint_path)\n","print(\"Training \" + model_name + \" checkpoints will be stored in the \" + checkpoint_path + \" folder\")\n","\n","if resume_from_checkpoint_path is not None:\n","     assert os.path.isdir(resume_from_checkpoint_path)\n","     print(\"Training \" + model_name + \" starting from the \" + resume_from_checkpoint_path + \" checkpoint\")\n","\n","assert 0 < learning_rate < 1\n","assert 1 <= train_batch_size_per_device <= len(train_dataset)\n","assert 1 <= val_batch_size_per_device <= len(val_dataset)\n","assert 1 <= epochs <= 10  # MobileBERT paper explains that they fine tune with 10 epochs max in section 4.4.2.\n","assert use_peft in [0, 1]\n","\n","# Getting array of tags/labels\n","finer_tag_names = train_dataset.features[\"ner_tags\"].feature.names\n","\n","# id2label and label2id dictionaries for loading the model.\n","id2label = {i: element for i, element in enumerate(finer_tag_names)}\n","label2id = {value: i for i, value in enumerate(finer_tag_names)}\n","\n","# Load the tokenizer and model object\n","if resume_from_checkpoint_path is not None:  # Load model and tokenizer from starting_checkpoint_path\n","     if use_peft == 1:\n","          from peft import PeftModel, PeftConfig\n","          config = PeftConfig.from_pretrained(resume_from_checkpoint_path)\n","          inference_model = AutoModelForTokenClassification.from_pretrained(\n","          config.base_model_name_or_path, num_labels=279, id2label=id2label, label2id=label2id)\n","          tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","          model = PeftModel.from_pretrained(inference_model, resume_from_checkpoint_path)\n","     else:\n","          model = AutoModelForTokenClassification.from_pretrained(resume_from_checkpoint_path)\n","          tokenizer = AutoTokenizer.from_pretrained(resume_from_checkpoint_path)\n","else:  # Load model from RQ3 utils functions\n","     from rq3_utils import return_model_object, return_model_tokenizer, tokenize_and_align_labels_mobilebert\n","     model = return_model_object(model_name, id2label, label2id, bnb_config)\n","     tokenizer = return_model_tokenizer(model_name)\n","\n","     if use_peft == 1: # PEFT config\n","          from rq3_utils import return_peft_config\n","          from peft import get_peft_model\n","          peft_config = return_peft_config(inference_mode=False)\n","          model = get_peft_model(model, peft_config)\n","     else:  # Only train the output/classification layer, freeze all other gradients\n","          for name, param in model.named_parameters():\n","               if \"classifier\" not in name:\n","                    param.requires_grad = False\n","\n","if use_peft == 1:\n","          print(model_name + \" PEFT parameter overview: \")\n","          model.print_trainable_parameters()\n","else:\n","     print(model_name, \"Total Parameter Count: \", model.num_parameters())\n","     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","     print(model_name, \"Trainable Parameter Count: \", str(trainable_params))\n","\n","\n","# Tokenize the train and val sets\n","if model_name == \"MobileBERT\":\n","     from rq3_utils import tokenize_and_align_labels_mobilebert\n","     tokenize_and_align_fn = tokenize_and_align_labels_mobilebert\n","elif model_name == \"SEC-BERT-BASE\":\n","     from rq3_utils import tokenize_and_align_labels_sec_bert_base\n","     tokenize_and_align_fn = tokenize_and_align_labels_sec_bert_base\n","elif model_name == \"SEC-BERT-NUM\":\n","     from rq3_utils import tokenize_and_align_labels_sec_bert_num\n","     tokenize_and_align_fn = tokenize_and_align_labels_sec_bert_num\n","elif model_name == \"SEC-BERT-SHAPE\":\n","     from rq3_utils import tokenize_and_align_labels_sec_bert_shape\n","     tokenize_and_align_fn = tokenize_and_align_labels_sec_bert_shape\n","\n","# NOTE: Tokenize and align for train takes about 15 mins with Colab L4 GPU\n","tokenized_train = train_dataset.map(tokenize_and_align_fn, batched=True)\n","tokenized_val = val_dataset.map(tokenize_and_align_fn, batched=True)\n","\n","# For creating batches of examples\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","\n","# Optimizer for each model.\n","if model_name == \"MobileBERT\":\n","     optimizer = torch_optimizer.Lamb(model.parameters(), lr=learning_rate)\n","else:  # SEC-BERT family\n","     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","training_with_checkpoint = False\n","if resume_from_checkpoint_path is not None:\n","     training_with_checkpoint = True\n","     # NOTE: automatically takes last checkpoint to train from.\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","     output_dir=checkpoint_path,\n","     per_device_train_batch_size=train_batch_size_per_device,\n","     per_device_eval_batch_size=val_batch_size_per_device,\n","     lr_scheduler_type=\"constant\",  # Disables LR scheduling which happens by default in HF\n","     num_train_epochs=epochs,\n","     evaluation_strategy=\"epoch\",\n","     save_strategy=\"epoch\",\n","     load_best_model_at_end=True,\n","     use_cpu=False\n",")\n","\n","# Model Trainer object\n","trainer = Trainer(\n","     model=model,\n","     optimizers=[optimizer, None],\n","     args=training_args,\n","     train_dataset=tokenized_train,\n","     eval_dataset=tokenized_val,\n","     tokenizer=tokenizer,\n","     data_collator=data_collator,\n","     compute_metrics=compute_metrics\n",")\n","\n","# Add callback to track training metrics\n","trainer.add_callback(CustomCallback(trainer))\n","\n","print(\"\\n\\n-----TRAINING-----\")\n","trainer.train(resume_from_checkpoint=training_with_checkpoint)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
